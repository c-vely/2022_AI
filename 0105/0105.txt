**** 파이썬 프로그램 자동화
- 납품 & 유지보수


*** 소스코드 생성 > 파일 이관
- 쥬피터 > File > Export Notebook to Executable Script > 다운로드 폴더에 프로그램 다운 됨
- d 드라이브 이동 > webProject 폴더 생성 > 작업폴더 복사(codeset, dataset, extern, imageset) > 만든 폴더에 붙여넣기
- codeset에는 webscrapy2.ipynb, webscrapy2.py 2개를 넣어둔다
- cmd > d: > cd webProject > dir/w > cd codeset > pyrhon webscrapy2.py 실행 > dataset에 csv파일 생성되면 성공


*** 실행파일 생성 및 테스트
- 메모장 실행 > 작성 > webscrapy.bat 이름으로 바탕화면에 저장 > 바탕화면 생성된 bat파일을 더블클릭 > dataset에 csv파일 만들어짐 > 실행 확인 완료

---------------------------------------
d:
cd d:\webProject \codeset
python webscrapy2.py
pause
---------------------------------------


*** 자동화 실행 (작업 스케줄러 사용하기)
- 제어판 > 시스템 및 보안 > 관리도구 > 작업스케쥴러 > 기본작업 만들기 클릭 
- 새 작업 만들기 > 이름, 설명 기재 > 트리거 > 매일, 시간 설정 > 동작 > 찾아보기 > 프로그램 : 바탕화면 bat 파일 클릭 > 실행 확인
-> 매일 같은 시간에 수집이 됨을 확인할 수 있음


* tip
- 서버 pc는 계속 켜둔다.  (서비스를 하는 사람을 서버라고 함)
- dataset폴더  :  검증 / 운영환경
- 내가 작업하는 곳  :  개발환경 (jupyter lab)
- 작업 스케줄러 > 내가 만든 작업 우클릭 > 실행 > 바로 실행되는 거 확인 가능


*** 수정
- 작업환경에서 코드 수정 후 >  다시 스크립트 다운 받아서 > d: codeset에 붙여넣기 하면 됨

* tip
- 파일 이름이 같아야 함





**** 공공데이터 수집
- 공공데이터포털    https://www.data.go.kr/          (오픈API 데이터 중 활용신청 하면 인증키가 발급됨)
- 국가통계포럼       https://kosis.kr/index/index.do    https://kosis.kr/openapi/index/index.jsp  (자동으로 됨)

- 캐글                  https://www.kaggle.com/        (사기데이터가 있으나 정확도 높은것도 있음)



*** 공공데이터포털 
- 한국주택금융공사_지역별 PIR 및 LIR.csv 다운로드 > 크롬 우측 3닷 버튼 > 다운로드목록 우클릭 > 링크복사 
- 쥬피터 > url 붙여 넣고 시작 >


** 깃허브에서 자료 가져올때
- https://github.com/hyokwan/python-lecture/blob/master/dataset/customerdata.csv

- customerdata.csv > 화면 상단 Raw 클릭 > 상단 url 복사 > 주피터에 붙여넣기


** 클립보드에서 자료 가져오기
- 자료 복사 > 쥬피터 > pd.read_clipboard( )

** 데이터베이스(psycopg2) 데이터 불러오기
- cmd > pip install psycopg2


* tip
- 웹 : utf-8  / 윈도우 : ms949???
- 데이터는 수집하되, 신뢰하지 못하는 데이터가 있어 꼭 전처리해야한다.


***코시스 Json 데이터 불러오기
- 활용신청 > 신청 완료
- 개발가이드 > 통계자료 > URL 생성 > 자료 검색 > 선택 > 분류 선택 > JSON > URL보기
- 쥬피터 > url 넣고 > 확인 


*** XML 테이터 불러오기 (난이도가 좀 있음)
- 공공데이터 > 국토교통부 아파트매매 실거래 검색 > 기술문서 다운로드 확인하기 
- 기술문서 상, 서비스개요 확인하고 > 요청 메세지 명세 확인(중요) 
- 활용신청 > 마이페이지 > 요청한 자료 클릭 > 일반 인증키 복사 > 주피터 붙여 넣기

--------------------------------------------
#Used to service API connection
import requests
from lxml import html
from bs4 import BeautifulSoup 
import pandas as pd
from urllib.request import Request, urlopen
from urllib.parse import urlencode, quote_plus, unquote

#한글이 있을경우 처리
API_Key = unquote("AT33s775KYpJOkUBJu0dxkJuUeIfDIOJRzAH084EQS3JN%2BzFjErLHuk%2FGZa9L4gBTSGCzeA69tI9PwLp7B37IQ%3D%3D")


url = "http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev"   // <--- 여기 내 인증키로 변경하면 됨
queryParams = '?' + urlencode(
    {
        quote_plus('ServiceKey') : API_Key,
        quote_plus('LAWD_CD'): '11110',
        quote_plus('DEAL_YMD'): '202212'
     }
)
response = requests.get(url+queryParams)
response.encoding='utf-8'
xmlobj = BeautifulSoup(response.text,"lxml-xml")
xmlobj
----------------------------------------------



**** 정리
* 1일차
- 빅데이터 개요 ( 3가지 직업군)
1. 사이언티스트 : 계속 고객이랑 얘기 함 (보통 1~2명)
2. 엔지니어 : 사이언티스트가 요청하는 자료를 만들어줌 / 일이 명확 (보통 10명)
3. 플랫폼 : 메뉴얼 잘 만드면 됨 / 고객이랑 안 만남 / 최신 테크 알아야 함 /  (보통 1~2명)

- 데이터분석 환경 (아나콘다, jupyter)


* 2일차
- 핵심 자료형(수, 문자, 리스트) / 문법 (반복문)


* 3일차
- 문법 (반복문 / 조건문 / 함수 / break(나를 감싸는 딱 1개만 탈출), continue(나 패스하고 다음으로 넘어감), pass(그냥 공백과 같음), try except(파일 불러오고나 읽거나, 웹사이트 접속하거나 할 때 사용)
- 데이터 수집 (requests, bs4)
- 셀레니움

--> 함수를 라이브러리로 만들어보는 것을 해보면 아주 좋음
--> 프로젝트를 많이 하다보면 함수목록들을 정리할 것이다. 원하는 명령의 함수를 가지고 있나? 새로 만들어야 하냐? 그렇게 확인할 수 있다. 



* 4일차
- 코드 패키징 및 유지보수 방법
- java jar(java archive) exe 패키징 / java war(web archive) tomcat 패키징 꼭 알기 
- 공공데이터 수집
  - 공공데이터포털, 국가통계포털 (FILE, JSON, XML데이터 수집 / GITHUB, CLIPBOARD, DB)

--> 고객은 자동으로 돌아가길 원한다. 
--> 꼭 미니 프로젝트 통해서 패키징까지 꼭 해보라.
--> 이정도면 웬만큼은 할 수 있고, 더 나아가면 이미지수집 +딥러닝 이용해서 하는 정도??



**** 내일 할일
- 예제코드및 데이터셋 자료 다운로드    https://o365kopo-my.sharepoint.com/personal/haiteam_office_kopo_ac_kr/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fhaiteam%5Foffice%5Fkopo%5Fac%5Fkr%2FDocuments%2FAttachments%2F00%2E%20%EC%97%85%EB%AC%B4%2Fxx%2E%20%EC%A3%BC%EC%9A%94%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8%2F%EB%8F%84%EC%84%9C%20%EC%9E%90%EB%A3%8C%EC%8B%A4%2F01%2E%20%EA%B8%B0%EB%B3%B8%ED%8E%B8&ga=1
- dataset 안 내용 복사 > stdCode/dataset 에 붙여넣기


*** 데이터 분석 및 데이터 전처리
- 데이터 유형을 파악해야 한다
- 절대 변하지 않는 데이터가 뭔지 확인하자


** 행 읽어 오기
- customerData.loc[ 조건식 ]











